//! AI Error Handling and Mock Tests
//!
//! OpenAIçµ±åˆã®ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã¨ãƒ¢ãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ

use mcp_rs::ai::llm::{ChatMessage, LlmClient};
use mcp_rs::ai::llm::openai::OpenAiClient;

#[tokio::test]
async fn test_generate_with_empty_prompt() {
    // ç©ºã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯å‡¦ç†ã‚’è©¦ã¿ã‚‹
    // ï¼ˆå®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã¯å¤±æ•—ã™ã‚‹ãŒã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã§ã¯å•é¡Œãªã„ï¼‰
    let client = OpenAiClient::new("test-key", "gpt-4");
    
    // generateé–¢æ•°è‡ªä½“ã¯æ­£å¸¸ã«æ§‹ç¯‰ã•ã‚Œã‚‹
    // APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ã¯å®Ÿéš›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã«ç™ºç”Ÿ
    let messages = vec![ChatMessage::user("")];
    
    // ãƒ¢ãƒƒã‚¯ãªã—ã®å ´åˆã€APIå‘¼ã³å‡ºã—ã¯å¤±æ•—ã™ã‚‹ã¯ãš
    // ã“ã“ã§ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ§‹ç¯‰ã®æ­£å¸¸æ€§ã®ã¿ãƒ†ã‚¹ãƒˆ
    assert_eq!(client.model_info().name, "gpt-4");
}

#[tokio::test]
async fn test_chat_with_empty_messages() {
    let client = OpenAiClient::new("test-key", "gpt-4");
    let empty_messages: Vec<ChatMessage> = vec![];
    
    // ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã§ã‚‚APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯æ§‹ç¯‰ã•ã‚Œã‚‹
    // å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã¯ã‚µãƒ¼ãƒãƒ¼å´ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŒã€
    // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã®æ§‹é€ ã¯æ­£å¸¸
    assert!(empty_messages.is_empty());
}

#[test]
fn test_invalid_api_key_format() {
    // ç„¡åŠ¹ãªAPIã‚­ãƒ¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚‚ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯æ§‹ç¯‰ã•ã‚Œã‚‹
    let invalid_keys = vec![
        "",
        "invalid",
        "sk-",
        "not-a-real-key",
    ];

    for key in invalid_keys {
        let client = OpenAiClient::new(key, "gpt-4");
        let info = client.model_info();
        assert_eq!(info.name, "gpt-4");
        // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ§‹ç¯‰è‡ªä½“ã¯æˆåŠŸã™ã‚‹ï¼ˆæ¤œè¨¼ã¯å®Ÿéš›ã®APIå‘¼ã³å‡ºã—æ™‚ï¼‰
    }
}

#[test]
fn test_model_name_variations() {
    let model_names = vec![
        "gpt-4",
        "gpt-4-32k",
        "gpt-3.5-turbo",
        "gpt-3.5-turbo-16k",
        "custom-model",
        "",
    ];

    for model_name in model_names {
        let client = OpenAiClient::new("test-key", model_name);
        let info = client.model_info();
        assert_eq!(info.name, model_name);
    }
}

#[tokio::test]
async fn test_health_check_with_invalid_key() {
    let client = OpenAiClient::new("invalid-key", "gpt-4");
    
    // ç„¡åŠ¹ãªã‚­ãƒ¼ã§ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã¯å¤±æ•—ã™ã‚‹ã¯ãš
    let result = client.health_check().await;
    
    // ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãŒã‚ã‚‹å ´åˆã¯401ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
    // ãªã„å ´åˆã¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
    // ã„ãšã‚Œã«ã—ã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèª
    assert!(result.is_err(), "Health check should fail with invalid key");
}

#[test]
fn test_client_with_extreme_parameters() {
    // æ¥µç«¯ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã§ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ§‹ç¯‰
    let client = OpenAiClient::new("test-key", "gpt-4")
        .with_max_tokens(0)
        .with_temperature(-1.0)
        .with_top_p(2.0);
    
    // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè‡ªä½“ã¯æ§‹ç¯‰ã•ã‚Œã‚‹ï¼ˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯APIå´ã§è¡Œã‚ã‚Œã‚‹ï¼‰
    let info = client.model_info();
    assert_eq!(info.name, "gpt-4");
}

#[test]
fn test_client_with_very_large_max_tokens() {
    let client = OpenAiClient::new("test-key", "gpt-4")
        .with_max_tokens(1_000_000);
    
    // éå¸¸ã«å¤§ããªmax_tokensã§ã‚‚æ§‹ç¯‰ã¯å¯èƒ½
    // å®Ÿéš›ã®APIå‘¼ã³å‡ºã—æ™‚ã«ã‚µãƒ¼ãƒãƒ¼å´ã§åˆ¶é™ã•ã‚Œã‚‹
    let info = client.model_info();
    assert_eq!(info.max_output_tokens, 4096); // ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®æœ€å¤§å€¤
}

#[test]
fn test_chat_message_with_special_characters() {
    let messages = vec![
        ChatMessage::user("Hello! @#$%^&*()"),
        ChatMessage::user("æ—¥æœ¬èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"),
        ChatMessage::user("Emoji: ğŸš€ğŸ‰"),
        ChatMessage::user("Newlines:\n\ntest"),
        ChatMessage::user("Quotes: \"test\" 'test'"),
    ];

    for msg in messages {
        assert_eq!(msg.role, "user");
        assert!(!msg.content.is_empty());
    }
}

#[test]
fn test_chat_message_with_long_content() {
    let long_content = "a".repeat(10000);
    let message = ChatMessage::user(&long_content);
    
    assert_eq!(message.role, "user");
    assert_eq!(message.content.len(), 10000);
}

#[test]
fn test_model_info_consistency() {
    // åŒã˜ãƒ¢ãƒ‡ãƒ«åã§è¤‡æ•°ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã—ã€
    // model_infoãŒä¸€è²«ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    let client1 = OpenAiClient::new("key1", "gpt-4");
    let client2 = OpenAiClient::new("key2", "gpt-4");
    
    let info1 = client1.model_info();
    let info2 = client2.model_info();
    
    assert_eq!(info1.name, info2.name);
    assert_eq!(info1.context_window, info2.context_window);
    assert_eq!(info1.cost_per_1k_tokens, info2.cost_per_1k_tokens);
}

#[test]
fn test_builder_pattern_chaining() {
    // ãƒ“ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒ¼ãƒ³ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    let client = OpenAiClient::new("test-key", "gpt-4")
        .with_max_tokens(1000)
        .with_temperature(0.8)
        .with_top_p(0.9)
        .with_base_url("https://custom.api.com/v1");
    
    let info = client.model_info();
    assert_eq!(info.name, "gpt-4");
    assert_eq!(info.provider, "OpenAI");
}

#[test]
fn test_multiple_chat_roles() {
    let messages = vec![
        ChatMessage::system("System prompt"),
        ChatMessage::user("User message 1"),
        ChatMessage::assistant("Assistant response 1"),
        ChatMessage::user("User message 2"),
        ChatMessage::assistant("Assistant response 2"),
        ChatMessage::user("User message 3"),
    ];

    assert_eq!(messages.len(), 6);
    
    // å½¹å‰²ã®é †åºãŒä¿æŒã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert_eq!(messages[0].role, "system");
    assert_eq!(messages[1].role, "user");
    assert_eq!(messages[2].role, "assistant");
}

#[test]
fn test_cost_calculation_accuracy() {
    let models_with_costs = vec![
        ("gpt-4", 0.03),
        ("gpt-4-32k", 0.06),
        ("gpt-3.5-turbo", 0.002),
        ("gpt-3.5-turbo-16k", 0.004),
    ];

    for (model, expected_cost) in models_with_costs {
        let client = OpenAiClient::new("test-key", model);
        let info = client.model_info();
        
        if let Some(cost) = info.cost_per_1k_tokens {
            assert_eq!(cost, expected_cost);
            
            // 10Kãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ã‚¹ãƒˆè¨ˆç®—ä¾‹
            let tokens_10k = 10000.0;
            let calculated_cost = (tokens_10k / 1000.0) * cost;
            assert!(calculated_cost > 0.0);
        }
    }
}

#[tokio::test]
async fn test_concurrent_client_creation() {
    // è¤‡æ•°ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä¸¦è¡Œã—ã¦ä½œæˆ
    let tasks = (0..10).map(|i| {
        tokio::spawn(async move {
            let client = OpenAiClient::new(
                format!("test-key-{}", i),
                "gpt-4"
            );
            client.model_info()
        })
    });

    let results = futures::future::join_all(tasks).await;
    
    for result in results {
        let info = result.unwrap();
        assert_eq!(info.name, "gpt-4");
        assert_eq!(info.provider, "OpenAI");
    }
}

#[test]
fn test_model_comparison_table() {
    // è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç”Ÿæˆ
    let models = vec!["gpt-4", "gpt-4-32k", "gpt-3.5-turbo", "gpt-3.5-turbo-16k"];
    
    for model in models {
        let client = OpenAiClient::new("test-key", model);
        let info = client.model_info();
        
        // å„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ã‚’æ¤œè¨¼
        assert!(!info.name.is_empty());
        assert!(info.context_window > 0);
        assert!(info.max_output_tokens > 0);
        
        // GPT-4ç³»ã¯GPT-3.5ã‚ˆã‚Šé«˜ã‚³ã‚¹ãƒˆ
        if model.starts_with("gpt-4") {
            assert!(info.cost_per_1k_tokens.unwrap_or(0.0) >= 0.03);
        } else {
            assert!(info.cost_per_1k_tokens.unwrap_or(0.0) <= 0.004);
        }
    }
}
