# LLMçµ±åˆã‚·ã‚¹ãƒ†ãƒ 

MCP-RSã®LLMçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã¯ã€è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAIã€Azure OpenAIã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼‰ã¨ã®çµ±åˆæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

- [æ¦‚è¦](#æ¦‚è¦)
- [æ©Ÿèƒ½](#æ©Ÿèƒ½)
- [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹](#apiãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹)
- [ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰](#ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰)
- [è¨­å®š](#è¨­å®š)
- [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

## æ¦‚è¦

LLMçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã¯ã€ä»¥ä¸‹ã®ä¸»è¦æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š

- **ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œ**: OpenAIã€Azure OpenAIã€ãƒ­ãƒ¼ã‚«ãƒ«LLM
- **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
- **ã‚»ã‚­ãƒ¥ã‚¢ãªAPI ã‚­ãƒ¼ç®¡ç†**: secrecyã‚¯ãƒ¬ãƒ¼ãƒˆã«ã‚ˆã‚‹å®‰å…¨ãªç®¡ç†
- **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆ**: å®Ÿè¡Œæ™‚ã®å‹•çš„åˆ‡ã‚Šæ›¿ãˆ
- **é«˜åº¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶å¾¡**: æ¸©åº¦ã€max_tokensã€top_pãªã©

## æ©Ÿèƒ½

### âœ… å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½

| æ©Ÿèƒ½ | èª¬æ˜ | çŠ¶æ…‹ |
|------|------|------|
| OpenAIçµ±åˆ | GPT-3.5/GPT-4ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒ¼ãƒˆ | âœ… |
| Azure OpenAIçµ±åˆ | ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå‘ã‘Azureçµ±åˆ | âœ… |
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ | âœ… |
| API ã‚­ãƒ¼ç®¡ç† | secrecyã«ã‚ˆã‚‹ã‚»ã‚­ãƒ¥ã‚¢ãªç®¡ç† | âœ… |
| ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆ | å®Ÿè¡Œæ™‚ã®å‹•çš„åˆ‡ã‚Šæ›¿ãˆ | âœ… |
| ç’°å¢ƒå¤‰æ•°ã‚µãƒãƒ¼ãƒˆ | `OPENAI_API_KEY`ã«ã‚ˆã‚‹è¨­å®š | âœ… |
| ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° | è©³ç´°ãªã‚¨ãƒ©ãƒ¼å‹å®šç¾© | âœ… |

### ğŸš§ ä»Šå¾Œã®æ©Ÿèƒ½

| æ©Ÿèƒ½ | èª¬æ˜ | å„ªå…ˆåº¦ |
|------|------|--------|
| ãƒ­ãƒ¼ã‚«ãƒ«LLMå¯¾å¿œ | llama.cppã€candleã‚µãƒãƒ¼ãƒˆ | é«˜ |
| ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | ä»»æ„ã®OpenAIäº’æ›API | ä¸­ |
| ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆ | tiktoken-rsã«ã‚ˆã‚‹äº‹å‰è¨ˆç®— | ä¸­ |
| ãƒ¬ãƒ¼ãƒˆåˆ¶é™ | ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥åˆ¶é™ç®¡ç† | ä½ |

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
src/llm/
â”œâ”€â”€ mod.rs              # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ«ãƒ¼ãƒˆ
â”œâ”€â”€ client.rs           # LlmClient: ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”œâ”€â”€ config.rs           # è¨­å®šç®¡ç†
â”œâ”€â”€ error.rs            # ã‚¨ãƒ©ãƒ¼å‹å®šç¾©
â”œâ”€â”€ types.rs            # å…±é€šå‹å®šç¾©
â”œâ”€â”€ streaming.rs        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ˜ãƒ«ãƒ‘ãƒ¼
â””â”€â”€ providers/
    â”œâ”€â”€ mod.rs          # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒˆãƒ¬ã‚¤ãƒˆ
    â””â”€â”€ openai.rs       # OpenAIå®Ÿè£…
```

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹æˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LlmClient                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Configuration (LlmConfig)        â”‚  â”‚
â”‚  â”‚  - provider: LlmProvider          â”‚  â”‚
â”‚  â”‚  - api_key: SecretString          â”‚  â”‚
â”‚  â”‚  - model: String                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Provider (trait LlmProvider)     â”‚  â”‚
â”‚  â”‚  - complete()                     â”‚  â”‚
â”‚  â”‚  - complete_stream()              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  OpenAI / Azure        â”‚
    â”‚  (async-openai)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

```rust
use mcp_rs::llm::{client::LlmClient, config::LlmConfig, types::Message};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 1. è¨­å®šã‚’ä½œæˆ
    let config = LlmConfig::openai("your-api-key", "gpt-3.5-turbo");
    
    // 2. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    let client = LlmClient::new(config)?;
    
    // 3. ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆå®Œäº†
    let response = client.complete_text("Hello, how are you?").await?;
    println!("Response: {}", response);
    
    Ok(())
}
```

### ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã®è¨­å®š

```rust
// OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•è¨­å®š
let config = LlmConfig::openai_from_env()?;
let client = LlmClient::new(config)?;
```

### ä¼šè©±å½¢å¼ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```rust
let messages = vec![
    Message::system("You are a helpful assistant"),
    Message::user("What is Rust?"),
];

let response = client.chat(messages).await?;
println!("Response: {}", response.content);
println!("Tokens used: {}", response.usage.total_tokens);
```

### ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãå®Œäº†

```rust
let response = client.complete_with_system(
    "You are a Rust expert",
    "Explain ownership in simple terms"
).await?;
```

### ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```rust
use mcp_rs::llm::{
    streaming::StreamHelper,
    types::LlmRequest,
};

let request = LlmRequest::new(messages).with_streaming(true);
let stream = client.complete_stream(request).await?;

// ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
let content = StreamHelper::process_stream(stream, |chunk| {
    print!("{}", chunk);
}).await?;
```

### ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```rust
let request = LlmRequest::new(messages)
    .with_temperature(0.8)      // å‰µé€ æ€§: 0.0-2.0
    .with_max_tokens(1000)      // æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    .with_model("gpt-4");       // ãƒ¢ãƒ‡ãƒ«æŒ‡å®š

let response = client.complete(request).await?;
```

## API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

### LlmConfig

```rust
pub struct LlmConfig {
    pub provider: LlmProvider,
    pub default_model: String,
    pub default_temperature: f32,
    pub default_max_tokens: usize,
    pub timeout_secs: u64,
    // ...
}
```

**ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰**:
- `openai(api_key, model)` - OpenAIè¨­å®šã‚’ä½œæˆ
- `azure_openai(api_key, endpoint, model)` - Azure OpenAIè¨­å®š
- `local(endpoint, model)` - ãƒ­ãƒ¼ã‚«ãƒ«LLMè¨­å®šï¼ˆäºˆå®šï¼‰
- `openai_from_env()` - ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®š
- `validate()` - è¨­å®šã‚’æ¤œè¨¼

### LlmClient

```rust
pub struct LlmClient { /* ... */ }
```

**ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰**:
- `new(config)` - æ–°ã—ã„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
- `complete(request)` - å®Œäº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
- `complete_text(prompt)` - ã‚·ãƒ³ãƒ—ãƒ«ãªå®Œäº†
- `complete_stream(request)` - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†
- `chat(messages)` - ä¼šè©±å½¢å¼ã®å®Œäº†
- `switch_provider(config)` - ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆ

### Message

```rust
pub struct Message {
    pub role: Role,
    pub content: String,
}
```

**ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰**:
- `Message::system(content)` - ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
- `Message::user(content)` - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
- `Message::assistant(content)` - ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

### LlmRequest

```rust
pub struct LlmRequest {
    pub messages: Vec<Message>,
    pub model: Option<String>,
    pub temperature: Option<f32>,
    pub max_tokens: Option<usize>,
    pub stream: bool,
}
```

**ãƒ“ãƒ«ãƒ€ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰**:
- `new(messages)` - ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
- `with_model(model)` - ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
- `with_temperature(temp)` - æ¸©åº¦ã‚’è¨­å®š
- `with_max_tokens(tokens)` - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨­å®š
- `with_streaming(stream)` - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–

### LlmResponse

```rust
pub struct LlmResponse {
    pub content: String,
    pub model: String,
    pub usage: TokenUsage,
    pub id: Option<String>,
    pub finish_reason: Option<String>,
}
```

## ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

### ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ

```rust
use mcp_rs::llm::{client::LlmClient, config::LlmConfig, types::Message};
use std::io::{self, Write};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = LlmConfig::openai_from_env()?;
    let client = LlmClient::new(config)?;
    
    let mut messages = vec![
        Message::system("You are a helpful assistant"),
    ];
    
    loop {
        print!("You: ");
        io::stdout().flush()?;
        
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;
        
        if input.trim() == "exit" {
            break;
        }
        
        messages.push(Message::user(input.trim()));
        
        let response = client.chat(messages.clone()).await?;
        println!("Assistant: {}", response.content);
        
        messages.push(Message::assistant(response.content));
    }
    
    Ok(())
}
```

### ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œã‚¢ãƒ—ãƒª

```rust
async fn create_client(provider_type: &str) -> Result<LlmClient, Box<dyn std::error::Error>> {
    let config = match provider_type {
        "openai" => LlmConfig::openai_from_env()?,
        "azure" => {
            let api_key = std::env::var("AZURE_OPENAI_API_KEY")?;
            let endpoint = std::env::var("AZURE_OPENAI_ENDPOINT")?;
            LlmConfig::azure_openai(api_key, endpoint, "gpt-4")
        },
        _ => return Err("Unknown provider".into()),
    };
    
    Ok(LlmClient::new(config)?)
}
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```rust
match client.complete_text("Hello").await {
    Ok(response) => println!("Success: {}", response),
    Err(LlmError::ApiError(msg)) => eprintln!("API Error: {}", msg),
    Err(LlmError::RateLimitError(msg)) => eprintln!("Rate limit: {}", msg),
    Err(LlmError::Timeout(secs)) => eprintln!("Timeout after {}s", secs),
    Err(e) => eprintln!("Error: {}", e),
}
```

## è¨­å®š

### ç’°å¢ƒå¤‰æ•°

| å¤‰æ•°å | èª¬æ˜ | å¿…é ˆ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
|--------|------|------|-----------|
| `OPENAI_API_KEY` | OpenAI APIã‚­ãƒ¼ | âœ“ | - |
| `OPENAI_MODEL` | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ« | | `gpt-3.5-turbo` |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI APIã‚­ãƒ¼ | Azureä½¿ç”¨æ™‚ | - |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | Azureä½¿ç”¨æ™‚ | - |

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ç¯„å›² | æ¨å¥¨å€¤ | èª¬æ˜ |
|-----------|------|--------|------|
| temperature | 0.0 - 2.0 | 0.7 | é«˜ã„ã»ã©å‰µé€ çš„ |
| max_tokens | 1 - 100000 | 2048 | ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•° |
| top_p | 0.0 - 1.0 | 1.0 | æ ¸ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| timeout_secs | 1 - 300 | 60 | ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰ |

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### 1. API ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„

**ã‚¨ãƒ©ãƒ¼**: `ConfigError: OPENAI_API_KEY not set`

**è§£æ±ºç­–**:
```bash
# ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
export OPENAI_API_KEY="your-api-key"

# ã¾ãŸã¯.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
echo "OPENAI_API_KEY=your-api-key" > .env
```

#### 2. ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼

**ã‚¨ãƒ©ãƒ¼**: `RateLimitError: Rate limit exceeded`

**è§£æ±ºç­–**:
- ãƒªã‚¯ã‚¨ã‚¹ãƒˆé »åº¦ã‚’ä¸‹ã’ã‚‹
- æœ‰æ–™ãƒ—ãƒ©ãƒ³ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
- ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…

```rust
use tokio::time::{sleep, Duration};

for retry in 0..3 {
    match client.complete_text("Hello").await {
        Ok(response) => return Ok(response),
        Err(LlmError::RateLimitError(_)) if retry < 2 => {
            sleep(Duration::from_secs(2u64.pow(retry))).await;
            continue;
        }
        Err(e) => return Err(e),
    }
}
```

#### 3. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

**ã‚¨ãƒ©ãƒ¼**: `Timeout: Request timeout after 60s`

**è§£æ±ºç­–**:
```rust
let mut config = LlmConfig::openai_from_env()?;
config.timeout_secs = 120; // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’120ç§’ã«å»¶é•·
```

#### 4. ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™è¶…é

**ã‚¨ãƒ©ãƒ¼**: `TokenLimitExceeded: requested 5000, max 4096`

**è§£æ±ºç­–**:
```rust
let request = LlmRequest::new(messages)
    .with_max_tokens(2000);  // ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
```

## ãƒ‡ãƒ¢å®Ÿè¡Œ

```bash
# LLMçµ±åˆãƒ‡ãƒ¢ã‚’å®Ÿè¡Œï¼ˆAPI ã‚­ãƒ¼ä¸è¦ã®ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ï¼‰
cargo run --example llm_integration_demo --features llm-integration

# å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã‚’å«ã‚€ãƒ‡ãƒ¢
export OPENAI_API_KEY="your-api-key"
cargo run --example llm_integration_demo --features llm-integration
```

## ãƒ†ã‚¹ãƒˆ

```bash
# ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
cargo test --features llm-integration --test llm_integration_test

# ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
cargo test --features llm-integration
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

| æ“ä½œ | å¹³å‡æ™‚é–“ | å‚™è€ƒ |
|------|---------|------|
| ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– | < 1ms | åˆå›ã®ã¿ |
| é€šå¸¸ã®å®Œäº† | 1-3s | ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ä¾å­˜ |
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹ | < 500ms | æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ |

### æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ

1. **æ¥ç¶šã®å†åˆ©ç”¨**: `LlmClient`ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†åˆ©ç”¨
2. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°**: é•·ã„å¿œç­”ã«ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ä½¿ç”¨
3. **max_tokensåˆ¶é™**: ä¸è¦ãªé•·ã„å¿œç­”ã‚’é¿ã‘ã‚‹
4. **ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆ**: ç‹¬ç«‹ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ä¸¦åˆ—å®Ÿè¡Œ

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

### API ã‚­ãƒ¼ç®¡ç†

- âœ… `secrecy`ã‚¯ãƒ¬ãƒ¼ãƒˆã«ã‚ˆã‚‹è‡ªå‹•ã‚¼ãƒ­åŒ–
- âœ… ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºæ™‚ã®è‡ªå‹•é™¤å¤–
- âœ… ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã§ã®ãƒã‚¹ã‚­ãƒ³ã‚°

### ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. API ã‚­ãƒ¼ã‚’ã‚³ãƒ¼ãƒ‰ã«ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã—ãªã„
2. ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯å®‰å…¨ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
3. æœ¬ç•ªç’°å¢ƒã§ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
4. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«æ©Ÿå¯†æƒ…å ±ã‚’å«ã‚ãªã„

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT OR Apache-2.0

## é–¢é€£ãƒªãƒ³ã‚¯

- [Issue #49 - LLMçµ±åˆã‚·ã‚¹ãƒ†ãƒ é–‹ç™º](https://github.com/n-takatsu/mcp-rs/issues/49)
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [async-openai crate](https://docs.rs/async-openai/)
