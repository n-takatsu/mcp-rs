//! WebSocket LLM Chat Server Example
//!
//! LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’æŒã¤ãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒãƒ¼ã®ã‚µãƒ³ãƒ—ãƒ«
//!
//! ## å®Ÿè¡Œæ–¹æ³•
//! ```bash
//! cargo run --example websocket_llm_chat
//! ```
//!
//! ## æ¥ç¶šæ–¹æ³•
//! ```javascript
//! const ws = new WebSocket('ws://localhost:8081/chat');
//! ws.onmessage = (event) => {
//!     const data = JSON.parse(event.data);
//!     console.log('Chunk:', data.content);
//! };
//! ws.send(JSON.stringify({ message: 'Hello, AI!' }));
//! ```

use axum::{
    extract::{
        ws::{Message, WebSocket, WebSocketUpgrade},
        State,
    },
    response::IntoResponse,
    routing::get,
    Router,
};
use mcp_rs::transport::websocket::{
    CompressionAlgorithm, CompressionConfig, CompressionManager, LlmStreamConfig, LlmStreamer,
    WebSocketMetrics,
};
use serde::{Deserialize, Serialize};
use std::net::SocketAddr;
use std::sync::Arc;
use tokio::time::{sleep, Duration};
use tracing::{error, info, warn};

/// ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
#[derive(Debug, Deserialize)]
struct ChatRequest {
    message: String,
    #[serde(default)]
    stream: bool,
}

/// ãƒãƒ£ãƒƒãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹
#[derive(Debug, Serialize)]
struct ChatResponse {
    #[serde(skip_serializing_if = "Option::is_none")]
    content: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    done: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    error: Option<String>,
}

/// ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹
#[derive(Clone)]
struct AppState {
    metrics: Arc<WebSocketMetrics>,
    llm_streamer: Arc<LlmStreamer>,
    compression: Arc<CompressionManager>,
}

#[tokio::main]
async fn main() {
    // ãƒ­ã‚®ãƒ³ã‚°åˆæœŸåŒ–
    tracing_subscriber::fmt::init();

    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆæœŸåŒ–
    let metrics = Arc::new(WebSocketMetrics::new().expect("Failed to create metrics"));

    // LLMã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼åˆæœŸåŒ–
    let llm_config = LlmStreamConfig {
        chunk_size: 20,
        delay_ms: 50,
        ..Default::default()
    };
    let llm_streamer = Arc::new(LlmStreamer::new(llm_config));

    // åœ§ç¸®ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–ï¼ˆBrotliä½¿ç”¨ï¼‰
    let compression_config = CompressionConfig {
        algorithm: CompressionAlgorithm::Brotli,
        level: 4,
        ..Default::default()
    };
    let compression = Arc::new(
        CompressionManager::new(compression_config).expect("Failed to create compression manager"),
    );

    // çŠ¶æ…‹åˆæœŸåŒ–
    let state = AppState {
        metrics,
        llm_streamer,
        compression,
    };

    // ãƒ«ãƒ¼ã‚¿ãƒ¼æ§‹ç¯‰
    let app = Router::new()
        .route("/chat", get(websocket_handler))
        .route("/health", get(health_check))
        .route("/metrics", get(metrics_handler))
        .with_state(state);

    // ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    let addr = SocketAddr::from(([127, 0, 0, 1], 8081));
    info!("ğŸ¤– WebSocket LLM Chat Server listening on {}", addr);
    info!("ğŸ“Š Health: http://localhost:8081/health");
    info!("ğŸ“ˆ Metrics: http://localhost:8081/metrics");
    info!("ğŸ’¬ Chat: ws://localhost:8081/chat");

    axum::Server::bind(&addr)
        .serve(app.into_make_service())
        .await
        .unwrap();
}

/// WebSocketã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
async fn websocket_handler(
    ws: WebSocketUpgrade,
    State(state): State<AppState>,
) -> impl IntoResponse {
    ws.on_upgrade(move |socket| handle_socket(socket, state))
}

/// WebSocketãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
async fn handle_socket(mut socket: WebSocket, state: AppState) {
    state.metrics.increment_connections();
    info!("âœ… New chat connection");

    // ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    let welcome = ChatResponse {
        content: Some("ğŸ¤– Welcome to LLM Chat! Send a message to start.".to_string()),
        done: Some(false),
        error: None,
    };
    let _ = send_json(&mut socket, &welcome).await;

    // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ—
    while let Some(msg) = socket.recv().await {
        match msg {
            Ok(Message::Text(text)) => {
                info!("ğŸ“¨ Received: {}", text);
                state.metrics.increment_messages_received();

                // JSONãƒ‘ãƒ¼ã‚¹
                match serde_json::from_str::<ChatRequest>(&text) {
                    Ok(request) => {
                        if request.stream {
                            // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                            handle_streaming_chat(&mut socket, &state, &request.message).await;
                        } else {
                            // é€šå¸¸ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                            handle_chat(&mut socket, &state, &request.message).await;
                        }
                    }
                    Err(e) => {
                        error!("Failed to parse request: {}", e);
                        let error_response = ChatResponse {
                            content: None,
                            done: Some(true),
                            error: Some(format!("Invalid JSON: {}", e)),
                        };
                        let _ = send_json(&mut socket, &error_response).await;
                        state.metrics.increment_errors();
                    }
                }
            }
            Ok(Message::Close(_)) => {
                info!("ğŸ‘‹ Client requested close");
                break;
            }
            Ok(_) => {
                // Ping/Pong/Binaryã¯ç„¡è¦–
            }
            Err(e) => {
                error!("WebSocket error: {}", e);
                state.metrics.increment_errors();
                break;
            }
        }
    }

    state.metrics.decrement_connections();
    info!("âŒ Chat connection closed");
}

/// é€šå¸¸ãƒãƒ£ãƒƒãƒˆå‡¦ç†
async fn handle_chat(socket: &mut WebSocket, state: &AppState, message: &str) {
    // ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸLLMå¿œç­”
    sleep(Duration::from_millis(500)).await;

    let response_text = format!(
        "You said: '{}'. This is a simulated response from an LLM model.",
        message
    );

    let response = ChatResponse {
        content: Some(response_text),
        done: Some(true),
        error: None,
    };

    if send_json(socket, &response).await.is_ok() {
        state.metrics.increment_messages_sent();
    } else {
        state.metrics.increment_errors();
    }
}

/// ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆå‡¦ç†
async fn handle_streaming_chat(socket: &mut WebSocket, state: &AppState, message: &str) {
    // ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸLLMå¿œç­”
    let full_response = format!(
        "You asked: '{}'. Here's a streaming response: \
         This demonstrates how the LLM Streamer works. \
         It breaks down long responses into smaller chunks \
         and sends them progressively to provide a better user experience. \
         The streaming approach allows users to see the response as it's being generated.",
        message
    );

    info!("ğŸ”„ Starting streaming response ({} chars)", full_response.len());

    // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹
    match state.llm_streamer.start_stream(&full_response).await {
        Ok(stream_id) => {
            let mut chunk_count = 0;

            // ãƒãƒ£ãƒ³ã‚¯ã‚’é †æ¬¡é€ä¿¡
            loop {
                match state.llm_streamer.next_chunk(&stream_id).await {
                    Ok(Some(chunk)) => {
                        chunk_count += 1;

                        let response = ChatResponse {
                            content: Some(chunk),
                            done: Some(false),
                            error: None,
                        };

                        if send_json(socket, &response).await.is_err() {
                            error!("Failed to send chunk {}", chunk_count);
                            state.metrics.increment_errors();
                            break;
                        }

                        state.metrics.increment_messages_sent();
                    }
                    Ok(None) => {
                        // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†
                        info!("âœ… Streaming complete ({} chunks)", chunk_count);

                        let done_response = ChatResponse {
                            content: None,
                            done: Some(true),
                            error: None,
                        };

                        let _ = send_json(socket, &done_response).await;
                        state.metrics.increment_messages_sent();
                        break;
                    }
                    Err(e) => {
                        error!("Streaming error: {}", e);
                        state.metrics.increment_errors();
                        break;
                    }
                }
            }

            // ã‚¹ãƒˆãƒªãƒ¼ãƒ çµ‚äº†
            let _ = state.llm_streamer.end_stream(&stream_id).await;
        }
        Err(e) => {
            error!("Failed to start stream: {}", e);
            let error_response = ChatResponse {
                content: None,
                done: Some(true),
                error: Some(format!("Streaming error: {}", e)),
            };
            let _ = send_json(socket, &error_response).await;
            state.metrics.increment_errors();
        }
    }
}

/// JSONå¿œç­”é€ä¿¡ï¼ˆåœ§ç¸®ä»˜ãï¼‰
async fn send_json<T: Serialize>(socket: &mut WebSocket, data: &T) -> Result<(), ()> {
    let json = serde_json::to_string(data).map_err(|e| {
        error!("Failed to serialize JSON: {}", e);
    })?;

    socket
        .send(Message::Text(json))
        .await
        .map_err(|e| {
            error!("Failed to send message: {}", e);
        })
}

/// ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
async fn health_check(State(state): State<AppState>) -> impl IntoResponse {
    let snapshot = state.metrics.snapshot();

    let status = serde_json::json!({
        "status": "healthy",
        "connections": snapshot.connections_total,
        "messages_sent": snapshot.messages_sent_total,
        "messages_received": snapshot.messages_received_total,
        "errors": snapshot.errors_total,
    });

    axum::Json(status)
}

/// Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
async fn metrics_handler(State(state): State<AppState>) -> impl IntoResponse {
    match state.metrics.export_text() {
        Ok(text) => (
            [(axum::http::header::CONTENT_TYPE, "text/plain")],
            text,
        )
            .into_response(),
        Err(e) => (
            axum::http::StatusCode::INTERNAL_SERVER_ERROR,
            format!("Failed to export metrics: {}", e),
        )
            .into_response(),
    }
}
